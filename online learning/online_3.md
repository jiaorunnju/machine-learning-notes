# Online Mirror Descent

## OMD

let $\psi$ be a regularizer function, $f_1,f_2,...$ be a sequence of loss functions played by nature.

On each iteration $t$, the learner choose weights:
$$
w_t = \arg\min_{w\in R^d} \psi(w)-w\theta_t
$$
where $z_t$ is the subgradient, and $\theta_t$ is the sum of first $t-1$ negative subgradient.

Note that we let the domain be $R^d$, since if has constraint, we can set the $\psi$ to be $\infty$ if $w$ violates constraint.

Note that

- OMD on $f_t$ is equivalent to FTRL on linear functions
- OGD is OMD with quadratic regularizer

Examples of regularizers:

- quadratic: $\frac{1}{2\eta}||w||_2^2$
- Non-spherical quadratic regularizer: $\frac{1}{2\eta}w^TAw$
- Entropic regularizer: $\psi(w)=\frac{1}{\eta}\sum w_i \log w_i$

## conjugate in OMD

for a function $\psi$, its conjuagte is
$$
\psi^*(\theta)=\sup_{w\in R^d}\{w\cdot \theta -\psi(w)\}
$$
since $\psi^*$ is linear then we have
$$
\nabla\psi^*(\theta)=\arg\max_{w\in R^d}\{w\cdot \theta - \psi(w)\}
$$
comparing this with updates in OMD, we can see that $w_t=\nabla\psi^*(\theta_t)$, and loss is $\psi^*(\theta_t)$

Consider optimality condition, we have $\theta_t=\nabla \psi(w_t)$

Now we have a one-to-one mapping between $w$ and $\theta$
$$
w_t=\nabla\psi^*(\theta_t) \\
\theta_t=\nabla \psi(w_t)
$$
The OMD takes a series of $\theta$, and $w$ is generated by duality.

## examples

### quadratic regularizer

$$
\psi(w)=\frac{1}{2\eta}||w||_2^2 \\
\psi^*(\theta)=\frac{\eta}{2}||\theta||_2^2\\
w = \eta\theta
$$

### entropy regularizer

$$
\psi(w)=\frac{1}{\eta}\sum_{i=1}^d w_i\log w_i \\
\psi^*(\theta) = \frac{1}{\eta}\log \left( \sum_{i=1}^d e^{\eta\theta_i} \right)\\
w_i=\frac{e^{\eta\theta_j}}{\sum_i^d e^{\eta\theta_i}}
$$